#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SUPERBID SCRAPER - OTIMIZADO PARA ML
âœ… Passive listening completo
âœ… 18 categorias principais
âœ… Estrutura enxuta focada em features para ML
"""

import sys
import json
import time
import requests
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional


class SuperbidScraper:
    """Scraper Superbid otimizado para anÃ¡lise ML"""
    
    def __init__(self):
        self.source = 'superbid'
        self.base_url = 'https://offer-query.superbid.net/seo/offers/'
        self.site_url = 'https://exchange.superbid.net'
        
        # 18 CATEGORIAS PRINCIPAIS
        self.categories = [
            ('alimentos-e-bebidas', 'Alimentos e Bebidas'),
            ('animais', 'Animais'),
            ('bolsas-canetas-joias-e-relogios', 'Bolsas, Canetas, Joias e RelÃ³gios'),
            ('caminhoes-onibus', 'CaminhÃµes e Ã”nibus'),
            ('carros-motos', 'Carros e Motos'),
            ('cozinhas-e-restaurantes', 'Cozinhas e Restaurantes'),
            ('eletrodomesticos', 'EletrodomÃ©sticos'),
            ('materiais-para-construcao-civil', 'Materiais para ConstruÃ§Ã£o Civil'),
            ('maquinas-pesadas-agricolas', 'MÃ¡quinas Pesadas e AgrÃ­colas'),
            ('industrial-maquinas-equipamentos', 'Industrial, MÃ¡quinas e Equipamentos'),
            ('imoveis', 'ImÃ³veis'),
            ('embarcacoes-aeronaves', 'EmbarcaÃ§Ãµes e Aeronaves'),
            ('moveis-e-decoracao', 'MÃ³veis e DecoraÃ§Ã£o'),
            ('movimentacao-transporte', 'MovimentaÃ§Ã£o e Transporte'),
            ('oportunidades', 'Oportunidades'),
            ('partes-e-pecas', 'Partes e PeÃ§as'),
            ('sucatas-materiais-residuos', 'Sucatas, Materiais e ResÃ­duos'),
            ('tecnologia', 'Tecnologia'),
        ]
        
        self.stats = {
            'total_scraped': 0,
            'by_category': {},
            'duplicates': 0,
            'with_bids': 0,
            'errors': 0,
        }
        
        self.headers = {
            "accept": "*/*",
            "accept-language": "pt-BR,pt;q=0.9",
            "origin": "https://exchange.superbid.net",
            "referer": "https://exchange.superbid.net/",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def scrape(self) -> List[Dict]:
        """Scrape completo de todas as categorias"""
        print("\n" + "="*80)
        print("ğŸ”µ SUPERBID - SCRAPER OTIMIZADO PARA ML")
        print("="*80)
        print(f"ğŸ“¦ Categorias: {len(self.categories)}")
        print("ğŸ¯ Foco: Campos essenciais para anÃ¡lise e oportunidades")
        print("="*80 + "\n")
        
        all_items = []
        global_ids = set()
        
        for idx, (url_slug, display_name) in enumerate(self.categories, 1):
            print(f"\n[{idx}/{len(self.categories)}] ğŸ“¦ {display_name}")
            print(f"{'â”€'*80}")
            
            category_items = self._scrape_category(
                url_slug, display_name, global_ids
            )
            
            all_items.extend(category_items)
            self.stats['by_category'][display_name] = len(category_items)
            
            print(f"   âœ… {len(category_items)} itens coletados")
            
            time.sleep(2)
        
        self.stats['total_scraped'] = len(all_items)
        return all_items
    
    def _scrape_category(self, url_slug: str, display_name: str, 
                        global_ids: set) -> List[Dict]:
        """Scrape completo de uma categoria (todas as pÃ¡ginas)"""
        items = []
        page_num = 1
        page_size = 100
        consecutive_errors = 0
        max_errors = 3
        
        while True:
            try:
                params = {
                    "urlSeo": f"{self.site_url}/categorias/{url_slug}",
                    "locale": "pt_BR",
                    "orderBy": "score:desc",
                    "pageNumber": page_num,
                    "pageSize": page_size,
                    "portalId": "[2,15]",
                    "requestOrigin": "marketplace",
                    "searchType": "opened" if url_slug == 'imoveis' else "openedAll",
                    "timeZoneId": "America/Sao_Paulo",
                }
                
                response = self.session.get(
                    self.base_url,
                    params=params,
                    timeout=30
                )
                
                if response.status_code != 200:
                    consecutive_errors += 1
                    print(f"   âš ï¸  Erro HTTP {response.status_code} na pÃ¡gina {page_num}")
                    if consecutive_errors >= max_errors:
                        break
                    page_num += 1
                    time.sleep(3)
                    continue
                
                data = response.json()
                offers = data.get('offers', [])
                total_offers = data.get('total', 0)
                
                if not offers:
                    break
                
                consecutive_errors = 0
                print(f"   ğŸ“„ PÃ¡gina {page_num}: {len(offers)} ofertas (total: {total_offers})")
                
                page_items = 0
                for offer_data in offers:
                    item = self._parse_offer(offer_data, display_name)
                    
                    if item and item['external_id'] not in global_ids:
                        items.append(item)
                        global_ids.add(item['external_id'])
                        page_items += 1
                        
                        if item.get('has_bids'):
                            self.stats['with_bids'] += 1
                    elif item:
                        self.stats['duplicates'] += 1
                
                # Verifica se hÃ¡ mais pÃ¡ginas
                start = data.get('start', 0)
                limit = data.get('limit', page_size)
                if start + limit >= total_offers:
                    break
                
                page_num += 1
                time.sleep(1)
                
            except Exception as e:
                consecutive_errors += 1
                self.stats['errors'] += 1
                print(f"   âš ï¸  Erro: {str(e)[:100]}")
                if consecutive_errors >= max_errors:
                    break
                page_num += 1
                time.sleep(3)
        
        return items
    
    def _parse_offer(self, offer: Dict, category_display: str) -> Optional[Dict]:
        """Parse - preserva raw_data completo"""
        try:
            offer_id = offer.get('id')
            if not offer_id:
                return None
            
            return {
                'external_id': f"superbid_{offer_id}",
                'category_display': category_display,
                'scraped_at': datetime.now().isoformat(),
                'raw_data': offer,  # TODOS os dados da API
                'offer_id': offer_id,
                'has_bids': offer.get('hasBids', False),
                'link': f"https://exchange.superbid.net/oferta/{offer_id}",
            }
            
        except Exception as e:
            self.stats['errors'] += 1
            return None
    
    def save(self, items: List[Dict], output_dir: Path = None) -> Path:
        """Salva dados coletados"""
        if output_dir is None:
            output_dir = Path(__file__).parent / 'data'
        
        output_dir.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        json_file = output_dir / f'superbid_{timestamp}.json'
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(items, f, ensure_ascii=False, indent=2)
        
        return json_file
    
    def print_stats(self):
        """Imprime estatÃ­sticas finais"""
        print("\n" + "="*80)
        print("ğŸ“Š ESTATÃSTICAS FINAIS")
        print("="*80)
        
        print(f"\nğŸ“¦ Por Categoria:")
        for category, count in sorted(self.stats['by_category'].items()):
            print(f"   â€¢ {category:<45} {count:>5} itens")
        
        print(f"\nğŸ“ˆ Resumo:")
        print(f"   â€¢ Total: {self.stats['total_scraped']}")
        print(f"   â€¢ Com lances: {self.stats['with_bids']}")
        print(f"   â€¢ Duplicatas: {self.stats['duplicates']}")
        print(f"   â€¢ Erros: {self.stats['errors']}")
        
        print("\n" + "="*80)


def main():
    """ExecuÃ§Ã£o principal"""
    print("\n" + "="*80)
    print("ğŸš€ SUPERBID - SCRAPER")
    print("="*80)
    print(f"ğŸ“… InÃ­cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    start_time = time.time()
    
    scraper = SuperbidScraper()
    items = scraper.scrape()
    
    if not items:
        print("\nâš ï¸  Nenhum item coletado")
        return 1
    
    json_file = scraper.save(items)
    print(f"\nğŸ’¾ Salvo: {json_file}")
    
    scraper.print_stats()
    
    elapsed = time.time() - start_time
    minutes = int(elapsed // 60)
    seconds = int(elapsed % 60)
    
    print(f"\nâ±ï¸  DuraÃ§Ã£o: {minutes}min {seconds}s")
    print(f"âœ… ConcluÃ­do: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())